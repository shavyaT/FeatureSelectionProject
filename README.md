
## FEATURE SELECTION
T2.1: Description of the dataset:
The DOROTHEA dataset was introduced in the NIPS 2003 Feature Selection Challenge. It is a binary classification task where each instance represents a compound tested for its binding affinity to a target site on the thrombin enzyme. Thrombin is a key enzyme involved in the blood clotting process, and identifying compounds that bind to it is crucial for drug discovery. 
 
The dataset comprises a total of 800 samples, each representing a compound. The compounds are assigned binary class labels based on their interaction with thrombin. Class +1 denotes compounds that bind to thrombin, comprising 722 samples, while Class -1 represents compounds that do not bind to thrombin, with only 78 samples in this category. The dataset is highly imbalanced, with just 9.26% of the compounds classified as Class +1.The summary in Table 1 suggests that the data is also highly sparse as even the most active feature has low intensity. The minimum mean zero suggests there are some features that are entirely inactive and are constant across all compounds.The left plot in figure 2, shows the distribution of active features per sample, peaking around ~1000, indicating significant sparsity. The right plot compares active feature distributions for binding and non-binding classes, revealing a broader distribution for the binding label, suggesting compounds that bind tend to have more active features. These graphs highlight the dataset's potential for dimensionality reduction, supporting the use of techniques like L1 regularization or RFE.
 
Preliminary filtering and Experimental Setup:
Removing the features that are always inactive or have 0 variance. This initial filtering decreases the dimensionality and removes uninformative features. 11,881 features were removed improving computational efficiency and ensuring that no completely useless features remain.The classifier used is Logistic Regression with `class_weight='balanced'`, which automatically adjusts the weight of the classes to address the imbalance in the dataset. This adjustment helps the model perform better in classifying the minority class (+1). For data splitting, stratified splits are employed to preserve the class ratio in both the training and testing sets. By using `stratify=y`, the training set accurately reflects the class imbalance present in the dataset, ensuring that the model learns patterns from a representative sample. 
T2.2: Three Feature Selection Approaches Covered in the Course
1) LASSO
We use LASSO for this task due to its ability to perform feature selection through L1 regularization, which reduces the number of features while preserving model accuracy. It automatically shrinks the coefficients of less important features to zero, making it ideal for high-dimensional datasets like DOROTHEA, where we need to minimize features without sacrificing predictive power. The code selects and evaluates features using a LASSO Logistic Regression model. With a maximum of 4000 iterations, the model is trained using penalty='l1', solver='saga', and C=0.01. After fitting, the number of iterations utilized is displayed to ensure convergence, which was 2657. After features with non-zero coefficients are chosen by SelectFromModel, a new Logistic Regression model is trained using these features. The cross-validation scores are [0.8984, 0.9358, 0.9462], suggesting satisfactory model performance. The test set has a balanced accuracy of 0.8419, with 18 features chosen.
2) Gradient Boosting
Given the large number of features in the DOROTHEA dataset, it is essential to focus on the most relevant ones to improve model performance and reduce computational complexity. The Gradient Boosting Classifier is a powerful non-linear model capable of uncovering subtle patterns in sparse and high-dimensional data. It ranks features based on their importance, enabling the detection of the most influential predictors. To determine the optimal number of features, we iteratively selected different subsets of features, (e.g. 50, 60, 70, 80, 90 and 100), based on their importance scores from the Gradient Boosting model. After evaluating each subset, we found that selecting the top 90 features yielded the best-balanced accuracy. The model's final balanced accuracy score of 0.812262 shows the effectiveness of this feature selection strategy.
3) Random Forest
Random Forest is selected for its capacity to assess feature importance using an ensemble of decision trees. It effectively handles high-dimensional datasets and ranks features based on their contribution to model performance, helping identify the most relevant features for the task while maintaining stability. The code builds a random forest model for feature selection, which is trained on scaled data with class weights set to balanced. It employs the 'SelectFromModel' function to select the most significant features based on their importance scores, with a limit of 50 features. After selecting the features, the model is retrained, and the balanced accuracy on the test set is determined to be 0.8053. This method identifies essential features that influence model performance, which improves the Random Forest classifier's interpretability and efficiency.
T2.3: Three Feature Selection Approaches not covered in the Course
1) Mutual Information
Mutual Information (MI) is a powerful feature selection method, particularly effective for high-dimensional and binary sparse datasets like DOROTHEA, as it quantifies the dependency between each feature and the target variable. By ranking features based on their MI scores, the method identifies those that provide the most information about the target, ensuring the retention of meaningful patterns while discarding irrelevant ones. For this dataset, selecting the top 100 features based on MI significantly improved model performance, and achieved a balanced accuracy of 0.8831 with Logistic Regression. This demonstrates MI's ability to handle sparse data effectively by focusing on features that capture essential relationships, reducing noise, and enabling the model to generalize better despite the dataset's complexity.
2) Chi-square 
Chi-square is a straightforward and commonly used statistical method. Features that have a higher chi-square statistic with respect to the class label are considered more informative and help eliminate the features that are independent of the target. Using the SelectKBest function with score_func = chi2, it selects the top 100 features most strongly associated with the target variable, Utilizing the χ² test's ability to measure dependency between categorical data. Here the value of k = 100 was determined through iterative experimentation, selecting the number of features that yielded the highest balanced accuracy for the Logistic Regression model. The model's predictions are assessed through the Balanced Accuracy metric, achieving a score of 0.8739, which demonstrates its effectiveness in finding the patterns within the reduced feature space while accommodating the class distribution. 
3) RFE
Recursive Feature Elimination (RFE) is efficient in selecting the most informative features. RFE works by removing less important features, improving model efficiency by focusing only on the most critical variables, which is essential for the high-dimensional DOROTHEA dataset. This method uses Recursive Feature Elimination (RFE) using Logistic Regression as the basis estimator to identify the top 20 features from the dataset. It initially fits the RFE model to the scaled training data, after which it extracts the selected features depending on the RFE ranking. After selecting the features, it applies a Logistic Regression model to them and generates the balanced accuracy on the test set, which is 0.8021. Furthermore, the algorithm uses cross-validation to assess the model's performance using the chosen features, yielding an average cross-validation score of 0.9083. Finally, it displays the feature importances based on the RFE ranking and model coefficients.
 
 
 
 
Conclusion of Task 2:
Mutual Information and LASSO dominated in terms of balanced accuracy with a number of features ranging from 18 to 100 out of the original 100,000, indicating a strong noise filtering capability. Chi-Square also performed relatively well, slightly behind MI and LASSO. Methods like RFE and Random Forest delivered moderate results, but struggle when dealing with a high dimensionality feature space and low "signal-to-noise ratio". It is safe to say that for this particular dataset, methods that either incorporate direct statistical relationships to the target (MI, Chi-Square) or integrate feature selection into model training with strong regularization (LASSO) proved superior. The high-dimensional and sparse nature of DOROTHEA appears to favor approaches that quickly cut through noise. More complex model-based selection methods may require more extensive parameter tuning or additional steps to reach comparable performance.
